{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amkayhani/DSML24/blob/main/5_2_bert_tokenisation_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b2840d",
      "metadata": {
        "id": "46b2840d"
      },
      "source": [
        "# **BERT Tokenisation Example**\n",
        "This notebook demonstrates tokenisation, padding and attention mask using BERT with a simple example.\n",
        "\n",
        "**Key Topics Covered:**\n",
        "- Tokenisation\n",
        "- Subword Splitting\n",
        "- Special Tokens ([CLS], [SEP])\n",
        "- Padding\n",
        "- Attention Masks\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca17a553",
      "metadata": {
        "id": "ca17a553"
      },
      "source": [
        "## **Step 1: Install and Import Dependencies**\n",
        "We'll install the required libraries and import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f60cffc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f60cffc3",
        "outputId": "e85edde3-db11-4b99-f1cf-ad1f1901b6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9d0abd58",
      "metadata": {
        "id": "9d0abd58"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dce010c",
      "metadata": {
        "id": "5dce010c"
      },
      "source": [
        "## **Step 2: Load the IMDb Dataset**\n",
        "We'll use the IMDb dataset, which consists of movie reviews labeled as positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391d0ce9",
      "metadata": {
        "id": "391d0ce9"
      },
      "source": [
        "## **Step 5: Subword Splitting**\n",
        "Tokenisation converts raw text into numerical representations for the model.\n",
        "\n",
        "**Why Tokenisation?**\n",
        "- Breaks text into smaller components (tokens)\n",
        "- Converts words into subwords for better vocabulary handling\n",
        "- Assigns a unique numerical ID to each tokenBERT uses **WordPiece tokenisation**, which breaks rare words into subwords.\n",
        "This allows handling of unseen words more efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9e2d8bb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e2d8bb6",
        "outputId": "d84cac9b-c7ff-4cf8-ec78-be176273b659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I really enjoyed this movie. The story was engaging and the characters were well-developed.\n",
            "Tokenised: ['i', 'really', 'enjoyed', 'this', 'movie', '.', 'the', 'story', 'was', 'engaging', 'and', 'the', 'characters', 'were', 'well', '-', 'developed', '.']\n",
            "Token IDs: [1045, 2428, 5632, 2023, 3185, 1012, 1996, 2466, 2001, 11973, 1998, 1996, 3494, 2020, 2092, 1011, 2764, 1012]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text = 'I really enjoyed this movie. The story was engaging and the characters were well-developed.'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print('Original Text:', text)\n",
        "print('Tokenised:', tokens)\n",
        "print('Token IDs:', token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62dc87d8",
      "metadata": {
        "id": "62dc87d8"
      },
      "source": [
        "## **Step 6: Special Tokens ([CLS], [SEP])**\n",
        "- `[CLS]` marks the start of a sentence.\n",
        "- `[SEP]` is used to separate sentences or mark the end of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1e3a9c7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3a9c7b",
        "outputId": "41c33860-6c52-47bc-a442-3bbdc1fc7ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens with Special Tokens: ['[CLS]', 'i', 'really', 'enjoyed', 'this', 'movie', '.', 'the', 'story', 'was', 'engaging', 'and', 'the', 'characters', 'were', 'well', '-', 'developed', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "tokens_with_special = ['[CLS]'] + tokens + ['[SEP]']\n",
        "print('Tokens with Special Tokens:', tokens_with_special)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75b6d80f",
      "metadata": {
        "id": "75b6d80f"
      },
      "source": [
        "## **Step 7: Padding**\n",
        "Since models process text in batches, all input sequences must have the same length.\n",
        "Padding ensures that shorter sequences are extended to match the longest sequence in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cc71c2c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc71c2c4",
        "outputId": "4341a636-5106-490b-99f5-1ef3d0a09e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded Input IDs: [101, 1045, 2428, 5632, 2023, 3185, 1012, 1996, 2466, 2001, 11973, 1998, 1996, 3494, 2020, 2092, 1011, 2764, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "tokenized_output = tokenizer([text], padding='max_length', truncation=True, max_length=50)\n",
        "print('Padded Input IDs:', tokenized_output['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981ca95e",
      "metadata": {
        "id": "981ca95e"
      },
      "source": [
        "## **Step 8: Attention Masks**\n",
        "Attention masks help the model ignore padding tokens (`0s`) while processing meaningful text (`1s`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "486166cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "486166cf",
        "outputId": "210c1990-de03-41f3-fda6-4e454c23c3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print('Attention Mask:', tokenized_output['attention_mask'][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}